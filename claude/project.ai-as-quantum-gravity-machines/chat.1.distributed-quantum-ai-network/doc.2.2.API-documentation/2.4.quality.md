# Quality Assessment API Documentation
## Domain API 2.4

### Overview
The Quality Assessment API provides access to SpiceTime's ethical scoring and content quality measurement systems. It enables multi-dimensional quality assessment across different domains while maintaining transparent scoring metrics.

### Core Types

```graphql
type QualityMetrics {
  # Three-dimensional ethical space metrics
  ethical: EthicalPosition!
  # Content quality measurements
  content: ContentQuality!
  # Temporal relevance factors
  temporal: TemporalMetrics!
  # Aggregated scoring
  totalScore: Float!
  confidence: Float!
}

type EthicalPosition {
  # Core ethical dimensions
  openness: Float!
  accountability: Float!
  respect: Float!
  # Position metrics
  distanceFromIdeal: Float!
  confidenceInterval: Float!
  domain: Domain!
}

type ContentQuality {
  # Technical assessment
  accuracy: Float!
  completeness: Float!
  consistency: Float!
  # Usage metrics
  engagement: Float!
  relevance: Float!
  impact: Float!
}

type TemporalMetrics {
  age: Int!
  freshness: Float!
  updateFrequency: Float!
  relevanceDecay: Float!
}

# Domain-specific quality configurations
type DomainQualityConfig {
  id: ID!
  name: String!
  weights: DomainWeights!
  thresholds: QualityThresholds!
  reviewers: [ReviewerConfig!]!
}

# Review process configuration
type ReviewerConfig {
  type: ReviewerType!
  minConfidence: Float!
  requiredSpecializations: [String!]!
  compensationRate: Float!
}
```

### Quality Assessment Operations

```graphql
type Query {
  # Fetch quality metrics for content
  quality(contentId: ID!): QualityMetrics!
  
  # Get ethical position
  ethicalPosition(contentId: ID!): EthicalPosition!
  
  # Retrieve domain configurations
  domainConfig(domain: String!): DomainQualityConfig!
  
  # List qualified reviewers
  availableReviewers(
    domain: String!
    minConfidence: Float
  ): [ReviewerInfo!]!
}

type Mutation {
  # Update quality metrics
  updateQuality(
    contentId: ID!
    metrics: QualityInput!
  ): QualityMetrics!
  
  # Configure domain quality parameters
  configureDomain(
    domain: String!
    config: DomainConfigInput!
  ): DomainQualityConfig!
  
  # Initiate review process
  requestReview(
    contentId: ID!
    reviewConfig: ReviewRequestInput!
  ): ReviewProcess!
}

type Subscription {
  # Monitor quality changes
  qualityUpdated(contentId: ID!): QualityMetrics!
  
  # Track review process
  reviewProgressUpdated(processId: ID!): ReviewProgress!
}
```

### Input Types

```graphql
input QualityInput {
  ethical: EthicalInput
  content: ContentQualityInput
  temporal: TemporalInput
}

input EthicalInput {
  openness: Float!
  accountability: Float!
  respect: Float!
  confidence: Float!
}

input ReviewRequestInput {
  requiredReviewers: Int!
  minConfidence: Float!
  deadline: DateTime
  specializations: [String!]
}

input DomainConfigInput {
  weights: DomainWeightsInput!
  thresholds: ThresholdsInput!
  reviewerRequirements: ReviewerRequirementsInput!
}
```

### Error Types

```graphql
type QualityError {
  code: QualityErrorCode!
  message: String!
  details: JSON
}

enum QualityErrorCode {
  INSUFFICIENT_CONFIDENCE
  BELOW_THRESHOLD
  REVIEWER_UNAVAILABLE
  INVALID_METRICS
  CONFIGURATION_ERROR
}
```

### Example Queries

```graphql
# Fetch complete quality metrics
query GetContentQuality($contentId: ID!) {
  quality(contentId: $contentId) {
    ethical {
      openness
      accountability
      respect
      distanceFromIdeal
      confidence
    }
    content {
      accuracy
      completeness
      impact
    }
    temporal {
      freshness
      relevanceDecay
    }
    totalScore
    confidence
  }
}

# Update quality metrics
mutation UpdateQuality($contentId: ID!, $metrics: QualityInput!) {
  updateQuality(contentId: $contentId, metrics: $metrics) {
    totalScore
    confidence
  }
}
```

### Workflow Examples

1. Quality Assessment Process:
```graphql
# 1. Request review
mutation InitiateReview($contentId: ID!) {
  requestReview(
    contentId: $contentId,
    reviewConfig: {
      requiredReviewers: 3,
      minConfidence: 0.8,
      specializations: ["technical", "ethical"]
    }
  ) {
    id
    status
    assignedReviewers
  }
}

# 2. Monitor progress
subscription MonitorReview($processId: ID!) {
  reviewProgressUpdated(processId: $processId) {
    completedReviews
    averageConfidence
    preliminaryScore
  }
}

# 3. Get final quality metrics
query FinalQuality($contentId: ID!) {
  quality(contentId: $contentId) {
    totalScore
    confidence
  }
}
```

### Best Practices

1. Quality Assessment:
   - Always specify confidence thresholds
   - Use domain-specific configurations
   - Monitor review progress
   - Handle temporal decay

2. Error Handling:
   - Check confidence levels
   - Validate metric ranges
   - Handle reviewer availability
   - Monitor threshold violations

3. Performance:
   - Batch quality updates
   - Use appropriate subscription patterns
   - Cache stable metrics
   - Monitor review latency

*Note: This documentation covers the quality assessment aspects of SpiceTime. For complete system integration, refer to related API documentation sections.*